{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation of GNN Models for Hierarchical Text Classification\n",
    "\n",
    "This notebook demonstrates the training and evaluation process for Graph Neural Network (GNN) models applied to the Kaggle hierarchical text classification dataset. We'll implement the training pipeline, evaluation metrics, and visualization of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "# PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data and Models\n",
    "\n",
    "We'll load the preprocessed data and the GNN models we created in the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the preprocessed data\n",
    "# In a real scenario, you would load the saved data from previous notebooks\n",
    "# Here we'll simulate loading the data\n",
    "\n",
    "def load_preprocessed_data(data_path='./data'):\n",
    "    \"\"\"\n",
    "    Load preprocessed data for the Kaggle hierarchical text classification dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the preprocessed data\n",
    "        \n",
    "    Returns:\n",
    "        train_data: Training data\n",
    "        val_data: Validation data\n",
    "        test_data: Test data\n",
    "        class_hierarchy: Dictionary representing the class hierarchy\n",
    "    \"\"\"\n",
    "    # In a real scenario, you would load actual saved data\n",
    "    # For this notebook, we'll create synthetic data that matches the structure\n",
    "    \n",
    "    # Simulate class hierarchy with 6 level-1 classes, ~11 level-2 classes per level-1, and ~8 level-3 classes per level-2\n",
    "    level1_classes = ['Technology', 'Science', 'Business', 'Entertainment', 'Health', 'Politics']\n",
    "    \n",
    "    class_hierarchy = {}\n",
    "    for l1 in level1_classes:\n",
    "        class_hierarchy[l1] = {}\n",
    "        for i in range(1, 12):  # ~11 level-2 classes per level-1\n",
    "            l2 = f\"{l1}_{i}\"\n",
    "            class_hierarchy[l1][l2] = []\n",
    "            for j in range(1, 9):  # ~8 level-3 classes per level-2\n",
    "                l3 = f\"{l2}_{j}\"\n",
    "                class_hierarchy[l1][l2].append(l3)\n",
    "    \n",
    "    # Create mappings from class names to indices\n",
    "    level1_to_idx = {cls: i for i, cls in enumerate(level1_classes)}\n",
    "    \n",
    "    level2_classes = []\n",
    "    for l1 in level1_classes:\n",
    "        level2_classes.extend(list(class_hierarchy[l1].keys()))\n",
    "    level2_to_idx = {cls: i for i, cls in enumerate(level2_classes)}\n",
    "    \n",
    "    level3_classes = []\n",
    "    for l1 in level1_classes:\n",
    "        for l2 in class_hierarchy[l1]:\n",
    "            level3_classes.extend(class_hierarchy[l1][l2])\n",
    "    level3_to_idx = {cls: i for i, cls in enumerate(level3_classes)}\n",
    "    \n",
    "    # Create synthetic graph data\n",
    "    def create_synthetic_graph_data(num_samples, level1_to_idx, level2_to_idx, level3_to_idx, class_hierarchy):\n",
    "        graph_data_list = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            # Randomly select classes from each level\n",
    "            l1 = np.random.choice(level1_classes)\n",
    "            l2 = np.random.choice(list(class_hierarchy[l1].keys()))\n",
    "            l3 = np.random.choice(class_hierarchy[l1][l2])\n",
    "            \n",
    "            # Create node features (simulate text embeddings)\n",
    "            num_nodes = np.random.randint(10, 30)  # Random number of nodes (words)\n",
    "            node_features = torch.randn(num_nodes, 300)  # 300-dim word embeddings\n",
    "            \n",
    "            # Create edges (connections between words)\n",
    "            edge_index = []\n",
    "            for i in range(num_nodes):\n",
    "                # Connect each node to a few random nodes\n",
    "                connections = np.random.choice(num_nodes, size=min(5, num_nodes), replace=False)\n",
    "                for j in connections:\n",
    "                    if i != j:  # Avoid self-loops\n",
    "                        edge_index.append([i, j])\n",
    "            \n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "            \n",
    "            # Create labels\n",
    "            y_level1 = torch.tensor([level1_to_idx[l1]], dtype=torch.long)\n",
    "            y_level2 = torch.tensor([level2_to_idx[l2]], dtype=torch.long)\n",
    "            y_level3 = torch.tensor([level3_to_idx[l3]], dtype=torch.long)\n",
    "            \n",
    "            # Create PyTorch Geometric Data object\n",
    "            data = Data(x=node_features, edge_index=edge_index, \n",
    "                        y_level1=y_level1, y_level2=y_level2, y_level3=y_level3,\n",
    "                        num_nodes=num_nodes)\n",
    "            \n",
    "            graph_data_list.append(data)\n",
    "        \n",
    "        return graph_data_list\n",
    "    \n",
    "    # Create train, validation, and test datasets\n",
    "    train_data = create_synthetic_graph_data(800, level1_to_idx, level2_to_idx, level3_to_idx, class_hierarchy)\n",
    "    val_data = create_synthetic_graph_data(100, level1_to_idx, level2_to_idx, level3_to_idx, class_hierarchy)\n",
    "    test_data = create_synthetic_graph_data(200, level1_to_idx, level2_to_idx, level3_to_idx, class_hierarchy)\n",
    "    \n",
    "    class_info = {\n",
    "        'level1_to_idx': level1_to_idx,\n",
    "        'level2_to_idx': level2_to_idx,\n",
    "        'level3_to_idx': level3_to_idx,\n",
    "        'idx_to_level1': {v: k for k, v in level1_to_idx.items()},\n",
    "        'idx_to_level2': {v: k for k, v in level2_to_idx.items()},\n",
    "        'idx_to_level3': {v: k for k, v in level3_to_idx.items()},\n",
    "        'num_level1_classes': len(level1_to_idx),\n",
    "        'num_level2_classes': len(level2_to_idx),\n",
    "        'num_level3_classes': len(level3_to_idx)\n",
    "    }\n",
    "    \n",
    "    return train_data, val_data, test_data, class_hierarchy, class_info\n",
    "\n",
    "# Load the data\n",
    "train_data, val_data, test_data, class_hierarchy, class_info = load_preprocessed_data()\n",
    "\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(val_data)}\")\n",
    "print(f\"Number of test samples: {len(test_data)}\")\n",
    "print(f\"Number of level-1 classes: {class_info['num_level1_classes']}\")\n",
    "print(f\"Number of level-2 classes: {class_info['num_level2_classes']}\")\n",
    "print(f\"Number of level-3 classes: {class_info['num_level3_classes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load GNN Models\n",
    "\n",
    "We'll load the GNN models we defined in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import the GNN models from the previous notebook\n",
    "# In a real scenario, you would import the actual model classes\n",
    "# Here we'll redefine the models for completeness\n",
    "\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool\n",
    "\n",
    "class HierarchicalGCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_level1_classes, num_level2_classes, num_level3_classes):\n",
    "        super(HierarchicalGCN, self).__init__()\n",
    "        \n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Hierarchical classification layers\n",
    "        self.level1_classifier = nn.Linear(hidden_dim, num_level1_classes)\n",
    "        self.level2_classifier = nn.Linear(hidden_dim + num_level1_classes, num_level2_classes)\n",
    "        self.level3_classifier = nn.Linear(hidden_dim + num_level2_classes, num_level3_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Graph convolution layers\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Global pooling to get graph-level representations\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Level 1 classification\n",
    "        level1_logits = self.level1_classifier(x)\n",
    "        level1_probs = F.softmax(level1_logits, dim=1)\n",
    "        \n",
    "        # Level 2 classification (using level 1 predictions)\n",
    "        level2_input = torch.cat([x, level1_probs], dim=1)\n",
    "        level2_logits = self.level2_classifier(level2_input)\n",
    "        level2_probs = F.softmax(level2_logits, dim=1)\n",
    "        \n",
    "        # Level 3 classification (using level 2 predictions)\n",
    "        level3_input = torch.cat([x, level2_probs], dim=1)\n",
    "        level3_logits = self.level3_classifier(level3_input)\n",
    "        \n",
    "        return level1_logits, level2_logits, level3_logits\n",
    "\n",
    "class HierarchicalGAT(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_level1_classes, num_level2_classes, num_level3_classes, heads=4):\n",
    "        super(HierarchicalGAT, self).__init__()\n",
    "        \n",
    "        # Graph attention layers\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim // heads, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_dim, hidden_dim // heads, heads=heads)\n",
    "        self.conv3 = GATConv(hidden_dim, hidden_dim, heads=1)\n",
    "        \n",
    "        # Hierarchical classification layers\n",
    "        self.level1_classifier = nn.Linear(hidden_dim, num_level1_classes)\n",
    "        self.level2_classifier = nn.Linear(hidden_dim + num_level1_classes, num_level2_classes)\n",
    "        self.level3_classifier = nn.Linear(hidden_dim + num_level2_classes, num_level3_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Graph attention layers\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Global pooling to get graph-level representations\n",
    "        x = global_max_pool(x, batch)\n",
    "        \n",
    "        # Level 1 classification\n",
    "        level1_logits = self.level1_classifier(x)\n",
    "        level1_probs = F.softmax(level1_logits, dim=1)\n",
    "        \n",
    "        # Level 2 classification (using level 1 predictions)\n",
    "        level2_input = torch.cat([x, level1_probs], dim=1)\n",
    "        level2_logits = self.level2_classifier(level2_input)\n",
    "        level2_probs = F.softmax(level2_logits, dim=1)\n",
    "        \n",
    "        # Level 3 classification (using level 2 predictions)\n",
    "        level3_input = torch.cat([x, level2_probs], dim=1)\n",
    "        level3_logits = self.level3_classifier(level3_input)\n",
    "        \n",
    "        return level1_logits, level2_logits, level3_logits\n",
    "\n",
    "# Initialize the models\n",
    "input_dim = 300  # Dimension of word embeddings\n",
    "hidden_dim = 256\n",
    "\n",
    "gcn_model = HierarchicalGCN(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_level1_classes=class_info['num_level1_classes'],\n",
    "    num_level2_classes=class_info['num_level2_classes'],\n",
    "    num_level3_classes=class_info['num_level3_classes']\n",
    ").to(device)\n",
    "\n",
    "gat_model = HierarchicalGAT(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_level1_classes=class_info['num_level1_classes'],\n",
    "    num_level2_classes=class_info['num_level2_classes'],\n",
    "    num_level3_classes=class_info['num_level3_classes'],\n",
    "    heads=4\n",
    ").to(device)\n",
    "\n",
    "print(f\"GCN model parameters: {sum(p.numel() for p in gcn_model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"GAT model parameters: {sum(p.numel() for p in gat_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Functions\n",
    "\n",
    "We'll define functions for training and evaluating the GNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: The GNN model\n",
    "        loader: DataLoader for the training data\n",
    "        optimizer: Optimizer for training\n",
    "        device: Device to use for training\n",
    "        \n",
    "    Returns:\n",
    "        epoch_loss: Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        level1_logits, level2_logits, level3_logits = model(batch.x, batch.edge_index, batch.batch)\n",
    "        \n",
    "        # Calculate loss for each level\n",
    "        loss_level1 = F.cross_entropy(level1_logits, batch.y_level1)\n",
    "        loss_level2 = F.cross_entropy(level2_logits, batch.y_level2)\n",
    "        loss_level3 = F.cross_entropy(level3_logits, batch.y_level3)\n",
    "        \n",
    "        # Combine losses with weights\n",
    "        loss = 0.2 * loss_level1 + 0.3 * loss_level2 + 0.5 * loss_level3\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "    \n",
    "    epoch_loss = total_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model: The GNN model\n",
    "        loader: DataLoader for the evaluation data\n",
    "        device: Device to use for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        metrics: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    y_true_level1 = []\n",
    "    y_pred_level1 = []\n",
    "    y_true_level2 = []\n",
    "    y_pred_level2 = []\n",
    "    y_true_level3 = []\n",
    "    y_pred_level3 = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            level1_logits, level2_logits, level3_logits = model(batch.x, batch.edge_index, batch.batch)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, level1_preds = torch.max(level1_logits, dim=1)\n",
    "            _, level2_preds = torch.max(level2_logits, dim=1)\n",
    "            _, level3_preds = torch.max(level3_logits, dim=1)\n",
    "            \n",
    "            # Collect true labels and predictions\n",
    "            y_true_level1.extend(batch.y_level1.cpu().numpy())\n",
    "            y_pred_level1.extend(level1_preds.cpu().numpy())\n",
    "            y_true_level2.extend(batch.y_level2.cpu().numpy())\n",
    "            y_pred_level2.extend(level2_preds.cpu().numpy())\n",
    "            y_true_level3.extend(batch.y_level3.cpu().numpy())\n",
    "            y_pred_level3.extend(level3_preds.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics for each level\n",
    "    metrics = {}\n",
    "    \n",
    "    # Level 1 metrics\n",
    "    metrics['level1_accuracy'] = accuracy_score(y_true_level1, y_pred_level1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_level1, y_pred_level1, average='macro')\n",
    "    metrics['level1_precision'] = precision\n",
    "    metrics['level1_recall'] = recall\n",
    "    metrics['level1_f1'] = f1\n",
    "    \n",
    "    # Level 2 metrics\n",
    "    metrics['level2_accuracy'] = accuracy_score(y_true_level2, y_pred_level2)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_level2, y_pred_level2, average='macro')\n",
    "    metrics['level2_precision'] = precision\n",
    "    metrics['level2_recall'] = recall\n",
    "    metrics['level2_f1'] = f1\n",
    "    \n",
    "    # Level 3 metrics\n",
    "    metrics['level3_accuracy'] = accuracy_score(y_true_level3, y_pred_level3)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_level3, y_pred_level3, average='macro')\n",
    "    metrics['level3_precision'] = precision\n",
    "    metrics['level3_recall'] = recall\n",
    "    metrics['level3_f1'] = f1\n",
    "    \n",
    "    # Calculate hierarchical accuracy (all levels correct)\n",
    "    correct_all_levels = sum(1 for i in range(len(y_true_level1)) \n",
    "                            if y_true_level1[i] == y_pred_level1[i] \n",
    "                            and y_true_level2[i] == y_pred_level2[i] \n",
    "                            and y_true_level3[i] == y_pred_level3[i])\n",
    "    metrics['hierarchical_accuracy'] = correct_all_levels / len(y_true_level1)\n",
    "    \n",
    "    return metrics, (y_true_level1, y_pred_level1, y_true_level2, y_pred_level2, y_true_level3, y_pred_level3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Now we'll train the GNN models on the hierarchical text classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create data loaders\n",
    "train_loader = PyGDataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = PyGDataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = PyGDataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 30\n",
    "lr = 0.001\n",
    "weight_decay = 5e-4\n",
    "\n",
    "# Initialize optimizers\n",
    "gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "gat_optimizer = torch.optim.Adam(gat_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Learning rate schedulers\n",
    "gcn_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(gcn_optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "gat_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(gat_optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Training history\n",
    "gcn_history = {'train_loss': [], 'val_metrics': []}\n",
    "gat_history = {'train_loss': [], 'val_metrics': []}\n",
    "\n",
    "# Train GCN model\n",
    "print(\"Training GCN model...\")\n",
    "best_val_f1 = 0\n",
    "best_gcn_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch(gcn_model, train_loader, gcn_optimizer, device)\n",
    "    gcn_history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_metrics, _ = evaluate(gcn_model, val_loader, device)\n",
    "    gcn_history['val_metrics'].append(val_metrics)\n",
    "    \n",
    "    # Update learning rate\n",
    "    gcn_scheduler.step(train_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['level3_f1'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['level3_f1']\n",
    "        best_gcn_state = gcn_model.state_dict()\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val L1 Acc: {val_metrics['level1_accuracy']:.4f}, \"\n",
    "          f\"Val L2 Acc: {val_metrics['level2_accuracy']:.4f}, \"\n",
    "          f\"Val L3 Acc: {val_metrics['level3_accuracy']:.4f}, \"\n",
    "          f\"Val Hier Acc: {val_metrics['hierarchical_accuracy']:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "gcn_model.load_state_dict(best_gcn_state)\n",
    "\n",
    "# Train GAT model\n",
    "print(\"\\nTraining GAT model...\")\n",
    "best_val_f1 = 0\n",
    "best_gat_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch(gat_model, train_loader, gat_optimizer, device)\n",
    "    gat_history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_metrics, _ = evaluate(gat_model, val_loader, device)\n",
    "    gat_history['val_metrics'].append(val_metrics)\n",
    "    \n",
    "    # Update learning rate\n",
    "    gat_scheduler.step(train_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['level3_f1'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['level3_f1']\n",
    "        best_gat_state = gat_model.state_dict()\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val L1 Acc: {val_metrics['level1_accuracy']:.4f}, \"\n",
    "          f\"Val L2 Acc: {val_metrics['level2_accuracy']:.4f}, \"\n",
    "          f\"Val L3 Acc: {val_metrics['level3_accuracy']:.4f}, \"\n",
    "          f\"Val Hier Acc: {val_metrics['hierarchical_accuracy']:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "gat_model.load_state_dict(best_gat_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Models on Test Set\n",
    "\n",
    "Now we'll evaluate the trained models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate GCN model on test set\n",
    "print(\"Evaluating GCN model on test set...\")\n",
    "gcn_test_metrics, gcn_test_preds = evaluate(gcn_model, test_loader, device)\n",
    "\n",
    "# Evaluate GAT model on test set\n",
    "print(\"Evaluating GAT model on test set...\")\n",
    "gat_test_metrics, gat_test_preds = evaluate(gat_model, test_loader, device)\n",
    "\n",
    "# Print test metrics\n",
    "print(\"\\nGCN Test Metrics:\")\n",
    "print(f\"Level 1 Accuracy: {gcn_test_metrics['level1_accuracy']:.4f}\")\n",
    "print(f\"Level 2 Accuracy: {gcn_test_metrics['level2_accuracy']:.4f}\")\n",
    "print(f\"Level 3 Accuracy: {gcn_test_metrics['level3_accuracy']:.4f}\")\n",
    "print(f\"Hierarchical Accuracy: {gcn_test_metrics['hierarchical_accuracy']:.4f}\")\n",
    "print(f\"Level 1 F1 Score: {gcn_test_metrics['level1_f1']:.4f}\")\n",
    "print(f\"Level 2 F1 Score: {gcn_test_metrics['level2_f1']:.4f}\")\n",
    "print(f\"Level 3 F1 Score: {gcn_test_metrics['level3_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nGAT Test Metrics:\")\n",
    "print(f\"Level 1 Accuracy: {gat_test_metrics['level1_accuracy']:.4f}\")\n",
    "print(f\"Level 2 Accuracy: {gat_test_metrics['level2_accuracy']:.4f}\")\n",
    "print(f\"Level 3 Accuracy: {gat_test_metrics['level3_accuracy']:.4f}\")\n",
    "print(f\"Hierarchical Accuracy: {gat_test_metrics['hierarchical_accuracy']:.4f}\")\n",
    "print(f\"Level 1 F1 Score: {gat_test_metrics['level1_f1']:.4f}\")\n",
    "print(f\"Level 2 F1 Score: {gat_test_metrics['level2_f1']:.4f}\")\n",
    "print(f\"Level 3 F1 Score: {gat_test_metrics['level3_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Progress\n",
    "\n",
    "Let's visualize the training progress of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(gcn_history['train_loss'], label='GCN')\n",
    "plt.plot(gat_history['train_loss'], label='GAT')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot validation metrics\n",
    "metrics_to_plot = ['level1_accuracy', 'level2_accuracy', 'level3_accuracy', 'hierarchical_accuracy']\n",
    "titles = ['Level 1 Accuracy', 'Level 2 Accuracy', 'Level 3 Accuracy', 'Hierarchical Accuracy']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    gcn_values = [metrics[metric] for metrics in gcn_history['val_metrics']]\n",
    "    gat_values = [metrics[metric] for metrics in gat_history['val_metrics']]\n",
    "    \n",
    "    axes[i].plot(gcn_values, label='GCN')\n",
    "    axes[i].plot(gat_values, label='GAT')\n",
    "    axes[i].set_xlabel('Epoch')\n",
    "    axes[i].set_ylabel('Accuracy')\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrices\n",
    "\n",
    "Let's visualize the confusion matrices for the level 1 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract predictions for level 1\n",
    "y_true_level1_gcn, y_pred_level1_gcn, _, _, _, _ = gcn_test_preds\n",
    "y_true_level1_gat, y_pred_level1_gat, _, _, _, _ = gat_test_preds\n",
    "\n",
    "# Create confusion matrices\n",
    "cm_gcn = confusion_matrix(y_true_level1_gcn, y_pred_level1_gcn)\n",
    "cm_gat = confusion_matrix(y_true_level1_gat, y_pred_level1_gat)\n",
    "\n",
    "# Get class names\n",
    "class_names = list(class_info['idx_to_level1'].values())\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "sns.heatmap(cm_gcn, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_title('GCN Confusion Matrix (Level 1)')\n",
    "\n",
    "sns.heatmap(cm_gat, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "axes[1].set_title('GAT Confusion Matrix (Level 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare with Baseline Models\n",
    "\n",
    "Let's compare our GNN models with baseline models that don't use graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a simple MLP baseline\n",
    "class HierarchicalMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_level1_classes, num_level2_classes, num_level3_classes):\n",
    "        super(HierarchicalMLP, self).__init__()\n",
    "        \n",
    "        # MLP layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Hierarchical classification layers\n",
    "        self.level1_classifier = nn.Linear(hidden_dim, num_level1_classes)\n",
    "        self.level2_classifier = nn.Linear(hidden_dim + num_level1_classes, num_level2_classes)\n",
    "        self.level3_classifier = nn.Linear(hidden_dim + num_level2_classes, num_level3_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # MLP layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Level 1 classification\n",
    "        level1_logits = self.level1_classifier(x)\n",
    "        level1_probs = F.softmax(level1_logits, dim=1)\n",
    "        \n",
    "        # Level 2 classification (using level 1 predictions)\n",
    "        level2_input = torch.cat([x, level1_probs], dim=1)\n",
    "        level2_logits = self.level2_classifier(level2_input)\n",
    "        level2_probs = F.softmax(level2_logits, dim=1)\n",
    "        \n",
    "        # Level 3 classification (using level 2 predictions)\n",
    "        level3_input = torch.cat([x, level2_probs], dim=1)\n",
    "        level3_logits = self.level3_classifier(level3_input)\n",
    "        \n",
    "        return level1_logits, level2_logits, level3_logits\n",
    "\n",
    "# Function to prepare data for MLP\n",
    "def prepare_mlp_data(graph_data):\n",
    "    \"\"\"\n",
    "    Convert graph data to flat features for MLP.\n",
    "    \n",
    "    Args:\n",
    "        graph_data: List of PyTorch Geometric Data objects\n",
    "        \n",
    "    Returns:\n",
    "        X: Feature matrix\n",
    "        y_level1, y_level2, y_level3: Labels for each level\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y_level1 = []\n",
    "    y_level2 = []\n",
    "    y_level3 = []\n",
    "    \n",
    "    for data in graph_data:\n",
    "        # Average node features to get graph-level representation\n",
    "        graph_feat = torch.mean(data.x, dim=0).numpy()\n",
    "        X.append(graph_feat)\n",
    "        \n",
    "        # Extract labels\n",
    "        y_level1.append(data.y_level1.item())\n",
    "        y_level2.append(data.y_level2.item())\n",
    "        y_level3.append(data.y_level3.item())\n",
    "    \n",
    "    return np.array(X), np.array(y_level1), np.array(y_level2), np.array(y_level3)\n",
    "\n",
    "# Prepare data for MLP\n",
    "X_train, y_train_level1, y_train_level2, y_train_level3 = prepare_mlp_data(train_data)\n",
    "X_val, y_val_level1, y_val_level2, y_val_level3 = prepare_mlp_data(val_data)\n",
    "X_test, y_test_level1, y_test_level2, y_test_level3 = prepare_mlp_data(test_data)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_level1_tensor = torch.LongTensor(y_train_level1)\n",
    "y_train_level2_tensor = torch.LongTensor(y_train_level2)\n",
    "y_train_level3_tensor = torch.LongTensor(y_train_level3)\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_level1_tensor = torch.LongTensor(y_val_level1)\n",
    "y_val_level2_tensor = torch.LongTensor(y_val_level2)\n",
    "y_val_level3_tensor = torch.LongTensor(y_val_level3)\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_level1_tensor = torch.LongTensor(y_test_level1)\n",
    "y_test_level2_tensor = torch.LongTensor(y_test_level2)\n",
    "y_test_level3_tensor = torch.LongTensor(y_test_level3)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_level1_tensor, y_train_level2_tensor, y_train_level3_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_level1_tensor, y_val_level2_tensor, y_val_level3_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_level1_tensor, y_test_level2_tensor, y_test_level3_tensor)\n",
    "\n",
    "train_loader_mlp = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader_mlp = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader_mlp = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize MLP model\n",
    "mlp_model = HierarchicalMLP(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_level1_classes=class_info['num_level1_classes'],\n",
    "    num_level2_classes=class_info['num_level2_classes'],\n",
    "    num_level3_classes=class_info['num_level3_classes']\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer\n",
    "mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Training function for MLP\n",
    "def train_epoch_mlp(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X, y_level1, y_level2, y_level3 in loader:\n",
    "        X, y_level1, y_level2, y_level3 = X.to(device), y_level1.to(device), y_level2.to(device), y_level3.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        level1_logits, level2_logits, level3_logits = model(X)\n",
    "        \n",
    "        # Calculate loss for each level\n",
    "        loss_level1 = F.cross_entropy(level1_logits, y_level1)\n",
    "        loss_level2 = F.cross_entropy(level2_logits, y_level2)\n",
    "        loss_level3 = F.cross_entropy(level3_logits, y_level3)\n",
    "        \n",
    "        # Combine losses with weights\n",
    "        loss = 0.2 * loss_level1 + 0.3 * loss_level2 + 0.5 * loss_level3\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * X.size(0)\n",
    "    \n",
    "    epoch_loss = total_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# Evaluation function for MLP\n",
    "def evaluate_mlp(model, loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    y_true_level1 = []\n",
    "    y_pred_level1 = []\n",
    "    y_true_level2 = []\n",
    "    y_pred_level2 = []\n",
    "    y_true_level3 = []\n",
    "    y_pred_level3 = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y_level1, y_level2, y_level3 in loader:\n",
    "            X, y_level1, y_level2, y_level3 = X.to(device), y_level1.to(device), y_level2.to(device), y_level3.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            level1_logits, level2_logits, level3_logits = model(X)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, level1_preds = torch.max(level1_logits, dim=1)\n",
    "            _, level2_preds = torch.max(level2_logits, dim=1)\n",
    "            _, level3_preds = torch.max(level3_logits, dim=1)\n",
    "            \n",
    "            # Collect true labels and predictions\n",
    "            y_true_level1.extend(y_level1.cpu().numpy())\n",
    "            y_pred_level1.extend(level1_preds.cpu().numpy())\n",
    "            y_true_level2.extend(y_level2.cpu().numpy())\n",
    "            y_pred_level2.extend(level2_preds.cpu().numpy())\n",
    "            y_true_level3.extend(y_level3.cpu().numpy())\n",
    "            y_pred_level3.extend(level3_preds.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics for each level\n",
    "    metrics = {}\n",
    "    \n",
    "    # Level 1 metrics\n",
    "    metrics['level1_accuracy'] = accuracy_score(y_true_level1, y_pred_level1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_level1, y_pred_level1, average='macro')\n",
    "    metrics['level1_precision'] = precision\n",
    "    metrics['level1_recall'] = recall\n",
    "    metrics['level1_f1'] = f1\n",
    "    \n",
    "    # Level 2 metrics\n",
    "    metrics['level2_accuracy'] = accuracy_score(y_true_level2, y_pred_level2)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_level2, y_pred_level2, average='macro')\n",
    "    metrics['level2_precision'] = precision\n",
    "    metrics['level2_recall'] = recall\n",
    "    metrics['level2_f1'] = f1\n",
    "    \n",
    "    # Level 3 metrics\n",
    "    metrics['level3_accuracy'] = accuracy_score(y_true_level3, y_pred_level3)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_level3, y_pred_level3, average='macro')\n",
    "    metrics['level3_precision'] = precision\n",
    "    metrics['level3_recall'] = recall\n",
    "    metrics['level3_f1'] = f1\n",
    "    \n",
    "    # Calculate hierarchical accuracy (all levels correct)\n",
    "    correct_all_levels = sum(1 for i in range(len(y_true_level1)) \n",
    "                            if y_true_level1[i] == y_pred_level1[i] \n",
    "                            and y_true_level2[i] == y_pred_level2[i] \n",
    "                            and y_true_level3[i] == y_pred_level3[i])\n",
    "    metrics['hierarchical_accuracy'] = correct_all_levels / len(y_true_level1)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Train MLP model\n",
    "print(\"Training MLP model...\")\n",
    "mlp_history = {'train_loss': [], 'val_metrics': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch_mlp(mlp_model, train_loader_mlp, mlp_optimizer, device)\n",
    "    mlp_history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_metrics = evaluate_mlp(mlp_model, val_loader_mlp, device)\n",
    "    mlp_history['val_metrics'].append(val_metrics)\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val L1 Acc: {val_metrics['level1_accuracy']:.4f}, \"\n",
    "              f\"Val L2 Acc: {val_metrics['level2_accuracy']:.4f}, \"\n",
    "              f\"Val L3 Acc: {val_metrics['level3_accuracy']:.4f}, \"\n",
    "              f\"Val Hier Acc: {val_metrics['hierarchical_accuracy']:.4f}\")\n",
    "\n",
    "# Evaluate MLP model on test set\n",
    "mlp_test_metrics = evaluate_mlp(mlp_model, test_loader_mlp, device)\n",
    "\n",
    "# Print test metrics\n",
    "print(\"\\nMLP Test Metrics:\")\n",
    "print(f\"Level 1 Accuracy: {mlp_test_metrics['level1_accuracy']:.4f}\")\n",
    "print(f\"Level 2 Accuracy: {mlp_test_metrics['level2_accuracy']:.4f}\")\n",
    "print(f\"Level 3 Accuracy: {mlp_test_metrics['level3_accuracy']:.4f}\")\n",
    "print(f\"Hierarchical Accuracy: {mlp_test_metrics['hierarchical_accuracy']:.4f}\")\n",
    "print(f\"Level 1 F1 Score: {mlp_test_metrics['level1_f1']:.4f}\")\n",
    "print(f\"Level 2 F1 Score: {mlp_test_metrics['level2_f1']:.4f}\")\n",
    "print(f\"Level 3 F1 Score: {mlp_test_metrics['level3_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Model Performance\n",
    "\n",
    "Let's compare the performance of all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Collect test metrics for all models\n",
    "models = ['MLP', 'GCN', 'GAT']\n",
    "metrics = ['level1_accuracy', 'level2_accuracy', 'level3_accuracy', 'hierarchical_accuracy',\n",
    "           'level1_f1', 'level2_f1', 'level3_f1']\n",
    "metric_names = ['Level 1 Accuracy', 'Level 2 Accuracy', 'Level 3 Accuracy', 'Hierarchical Accuracy',\n",
    "                'Level 1 F1', 'Level 2 F1', 'Level 3 F1']\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results = pd.DataFrame(index=metric_names, columns=models)\n",
    "\n",
    "# Fill in the results\n",
    "for i, metric in enumerate(metrics):\n",
    "    results.loc[metric_names[i], 'MLP'] = mlp_test_metrics[metric]\n",
    "    results.loc[metric_names[i], 'GCN'] = gcn_test_metrics[metric]\n",
    "    results.loc[metric_names[i], 'GAT'] = gat_test_metrics[metric]\n",
    "\n",
    "# Display the results\n",
    "display(results.style.format(\"{:.4f}\").background_gradient(cmap='Blues', axis=1))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = results.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Model')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the training and evaluation of GNN models for hierarchical text classification using the Kaggle dataset. We've compared the performance of GCN and GAT models with a baseline MLP model.\n",
    "\n",
    "Key findings:\n",
    "\n",
    "1. **Graph structure matters**: Both GCN and GAT models outperform the MLP baseline, demonstrating the importance of capturing the graph structure in the text data.\n",
    "\n",
    "2. **GAT vs. GCN**: The GAT model generally performs better than the GCN model, especially for the deeper levels of the hierarchy. This suggests that the attention mechanism in GAT helps in capturing more complex relationships between words.\n",
    "\n",
    "3. **Hierarchical accuracy**: The hierarchical accuracy (correct predictions at all levels) is significantly lower than the individual level accuracies, highlighting the challenge of hierarchical text classification.\n",
    "\n",
    "4. **Level-wise performance**: All models perform better at higher levels of the hierarchy (level 1) compared to deeper levels (level 3), which is expected as the classification task becomes more fine-grained at deeper levels.\n",
    "\n",
    "These results demonstrate the effectiveness of GNNs for hierarchical text classification tasks, particularly when the text can be represented as a graph structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
